{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92713dd",
   "metadata": {},
   "source": [
    "# MBDA 770 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07739f8a",
   "metadata": {},
   "source": [
    "## David Curtis and Jehu Humphries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ce5de",
   "metadata": {},
   "source": [
    "# Intoduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477977ad",
   "metadata": {},
   "source": [
    "The dataset used in this project was downloaded from the UCI Machine Learning Repository and is a collection of handwritten letters used for image classification. The name of the dataset is \"Letter Recognition\" and was originally created in 1990 by David Slate. There are 20,000 instances in the data set each representing a handwritten english capital letters in 20 different fonts. Although the scope of this assignment is to conduct exploratory analysis and partition the data, efforts undertaken during this assingment will feed into the larger goals of the project. The goal of the project will be the creation of an image classification system that will iteratively improve itself through a simulated online learning environment. During this paper, the data will be explored for imbalances, variables inspected to understand the distibution of data, partitioned for training and testing, and partitioned for model improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246da20f",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f6102624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to gather data\n",
    "import ucimlrepo\n",
    "\n",
    "# used for exploratory analysis and data partitioning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Used for Data Partitioning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Used for model SGD training\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# used to create Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# used to create Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# used to create Passive Aggressive Model\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "# used for artificial nueral network model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# used for model evaluation\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "54629bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e8b0214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo, list_available_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491651a",
   "metadata": {},
   "source": [
    "## Import Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea30d0b",
   "metadata": {},
   "source": [
    "The code snippet below downloads the dataset from the machine learning repository through the UCI repository API and saves the data files in the local environment as two data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ab231b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset from UCI \n",
    "letter_recognition = fetch_ucirepo(id=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dfcbcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes) \n",
    "X = letter_recognition.data.features \n",
    "y = letter_recognition.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b644c",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bffd1dd",
   "metadata": {},
   "source": [
    "The features in the dataset are scaled statistical details extracted from each of the images and the the target variable is categorical variable containing each letter. The code snippet below outlines each of the variables in the two datasets used for this paper. Additionally, a description for each variable is avaiable in the code output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "16a8ca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name     role         type demographic                    description  \\\n",
      "0   lettr   Target  Categorical        None                 capital letter   \n",
      "1   x-box  Feature      Integer        None     horizontal position of box   \n",
      "2   y-box  Feature      Integer        None       vertical position of box   \n",
      "3   width  Feature      Integer        None                   width of box   \n",
      "4    high  Feature      Integer        None                  height of box   \n",
      "5   onpix  Feature      Integer        None              total # on pixels   \n",
      "6   x-bar  Feature      Integer        None     mean x of on pixels in box   \n",
      "7   y-bar  Feature      Integer        None     mean y of on pixels in box   \n",
      "8   x2bar  Feature      Integer        None                mean x variance   \n",
      "9   y2bar  Feature      Integer        None                mean y variance   \n",
      "10  xybar  Feature      Integer        None           mean x y correlation   \n",
      "11  x2ybr  Feature      Integer        None              mean of x * x * y   \n",
      "12  xy2br  Feature      Integer        None              mean of x * y * y   \n",
      "13  x-ege  Feature      Integer        None  mean edge count left to right   \n",
      "14  xegvy  Feature      Integer        None    correlation of x-ege with y   \n",
      "15  y-ege  Feature      Integer        None  mean edge count bottom to top   \n",
      "16  yegvx  Feature      Integer        None    correlation of y-ege with x   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n",
      "14  None             no  \n",
      "15  None             no  \n",
      "16  None             no  \n"
     ]
    }
   ],
   "source": [
    "# gather variable information \n",
    "print(letter_recognition.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "575a706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   x-box   20000 non-null  int64\n",
      " 1   y-box   20000 non-null  int64\n",
      " 2   width   20000 non-null  int64\n",
      " 3   high    20000 non-null  int64\n",
      " 4   onpix   20000 non-null  int64\n",
      " 5   x-bar   20000 non-null  int64\n",
      " 6   y-bar   20000 non-null  int64\n",
      " 7   x2bar   20000 non-null  int64\n",
      " 8   y2bar   20000 non-null  int64\n",
      " 9   xybar   20000 non-null  int64\n",
      " 10  x2ybr   20000 non-null  int64\n",
      " 11  xy2br   20000 non-null  int64\n",
      " 12  x-ege   20000 non-null  int64\n",
      " 13  xegvy   20000 non-null  int64\n",
      " 14  y-ege   20000 non-null  int64\n",
      " 15  yegvx   20000 non-null  int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract details from features data frame\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c92f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   lettr   20000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Extract details of target data frame\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0301ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head of target data frame\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head of features data frame\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47e0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data frames\n",
    "data = pd.merge(y, X, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef23536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head of merged data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2bdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tail of merged data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather summary statistics of data frame\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829023c",
   "metadata": {},
   "source": [
    "## Target Variable Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88088bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather details about possible imbalances to the target variable in the downloaded data\n",
    "data['lettr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the data set.\n",
    "category_counts = data['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908339e2",
   "metadata": {},
   "source": [
    "From the code above it is apparent that there is relatively balanced representation of each of the letters in the dataset. When partitioning data, the balance for the training, testing, and online learning data is expected to degrade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27838432",
   "metadata": {},
   "source": [
    "## \"X-Box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the distribution of data in each variable. The visualization below is the horizontal position of each letter.\n",
    "sns.boxplot(x = \"lettr\", y = \"x-box\", data = data, order = sorted(data[\"lettr\"].unique()))\n",
    "\n",
    "plt.title('Distribution of \"x-box\" Grouped by Letter')\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel('\"x-box\" Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7ae4d",
   "metadata": {},
   "source": [
    "The boxplot of the horizontal position in the image reveals details about how the pixels are colored in an image of each letter. Outliers are present in each of the boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686af9af",
   "metadata": {},
   "source": [
    "## \"Y-Box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ee464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the vertical position of each letter in the data frame. \n",
    "sns.boxplot(x = \"lettr\", y = \"y-box\", data = data, order = sorted(data[\"lettr\"].unique()))\n",
    "\n",
    "plt.title('Distribution of \"y-box\" Grouped by Letter')\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel('\"y-box\" Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0a280",
   "metadata": {},
   "source": [
    "The boxplot of the vertical position of each letter reveal no outliers but an incredibly large range of values. In a classification task, this may reveal a lower variable importance compared to other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b4110",
   "metadata": {},
   "source": [
    "## \"Width\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f48e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the width of each letter in the data frame. \n",
    "sns.boxplot(x = \"lettr\", y = \"width\", data = data, order = sorted(data[\"lettr\"].unique()))\n",
    "\n",
    "plt.title('Distribution of \"Width\" Grouped by Letter')\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel('\"Width\" Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec7080",
   "metadata": {},
   "source": [
    "Unsurprisingly, the letter I shows a large seperation between other letters in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31bc6e",
   "metadata": {},
   "source": [
    "## \"High\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1790313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the vertical position of each letter in the data frame. \n",
    "sns.boxplot(x = \"lettr\", y = \"high\", data = data, order = sorted(data[\"lettr\"].unique()))\n",
    "\n",
    "plt.title('Distribution of \"high\" Grouped by Letter')\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel('\"high\" Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503543ed",
   "metadata": {},
   "source": [
    "Several letters present interesting details of how they are commonly written based on the height of the letter in each instance. The letter \"Q\" generally is written most commonly larger than other letters so much so that it is the first presence of an outlier in the lower bounds of the IQR. Letters Z, Y, and J, have several very tall images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560ee79",
   "metadata": {},
   "source": [
    "## \"Onpix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the vertical position of each letter in the data frame. \n",
    "sns.boxplot(x = \"lettr\", y = \"onpix\", data = data, order = sorted(data[\"lettr\"].unique()))\n",
    "\n",
    "plt.title('Distribution of \"onpix\" Grouped by Letter')\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel('\"onpix\" Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971d3c7",
   "metadata": {},
   "source": [
    "The variable \"onpix\" represents the number of pixels in the image and is scaled for future classification. The remaining variables in the data frame are the result of mathematical analysis and are valuable to classification but are more complex than vertical position, horiztonal position, width, height, and number of pixels. While the remaining variables may serve as features that are important to classification, they do not easily communicate differences between the letters though boxplot visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd6a85",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8daf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix of the features in the data set\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c00a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix, annot = False, fmt = \".2f\", cmap = 'coolwarm',\n",
    "            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e191be",
   "metadata": {},
   "source": [
    "Unsurprisingly, the areas with the greatest positive correlation are found in the variables that describe the height, width, and number of pixels. It is reasonable that as the box used to segregate the handritten image from its background was gathered, the larger and wider the box became, the more pixels were caputered in the box. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d3c36",
   "metadata": {},
   "source": [
    "# Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05768974",
   "metadata": {},
   "source": [
    "The next portion of this paper will partition the data for training, testing, and learning. Although previous studies have created models based on 16,000 instances and validated the model on the remaining 4,000 instances, this project will partition data differently. The first point of seperation between previous studies is that the project will create a larger training partition by 5% compared to the amount used to validate the model, but will be 50% of the size of the other studies. This project will create a 80/20 partition on half of the data instead of the 75/25 split of all the data in other studies. The partition will first occur by seperating the model in two equal sized data frames. 50% of the data will be used to train and validate the model, the other 50% will serve as new information that will improve the model. It is expected that the intital models will suffer in accuracy because there are 20 different fonts used in the data set and some letters may not be expressed in each of the fonts. However, by updating the model with images of known letters written in unknown fonts, the model will improve. The seperation and incremental model improvement is a crucial aspect of a fluid ML system that can rapidly adjust to new environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27944aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data partition of 50% of the data. Data1 will be partitioned to a train/validate split and Data2 will be split 5 times for itertive improvement.\n",
    "data1, data2 = train_test_split(data, test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4534fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Data1 into a train/validate data split\n",
    "data_train, data_test = train_test_split(data1, test_size = .2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "432855ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Data2 into new data frames to later simulate new information\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "data_shuffled = data2.sample(frac=1, random_state=44).reset_index(drop=True)\n",
    "\n",
    "# Calculate the size of each partition\n",
    "partition_size = int(np.ceil(len(data_shuffled) / 5))\n",
    "\n",
    "# Split the data into 5 equally sized DataFrames\n",
    "data_parts = [data_shuffled.iloc[i * partition_size:(i + 1) * partition_size] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91412722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and rename the data frames from the list\n",
    "new1, new2, new3, new4, new5 = data_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78517c",
   "metadata": {},
   "source": [
    "## Class Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c72b1",
   "metadata": {},
   "source": [
    "Because the target variable represents 26 different letters in 20 fonts, there are opportuntites for imbalances and information that the model may not have been trained with. The primary concern is how each of the letters are represented in the training data. Ideally, each of the letters is present in the training data, but is written in a font that the model may not have seen and can be improved with when it encounters new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the training set.\n",
    "category_counts = data_train['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in Training Data')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d357181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the data set.\n",
    "category_counts = data_test['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in Validation Data')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the \"new data\".\n",
    "category_counts = new1['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in \"New Information 1\"')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the \"new data\".\n",
    "category_counts = new2['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in \"New Information 2\"')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027baf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the \"new data\".\n",
    "category_counts = new3['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in \"New Information 3\"')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the \"new data\".\n",
    "category_counts = new4['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in \"New Information 4\"')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the distribution of each letter in the \"new data\".\n",
    "category_counts = new5['lettr'].value_counts()\n",
    "\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Frequency of Letters in \"New Information 5\"')\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb5f23",
   "metadata": {},
   "source": [
    "Although there is very good balance of each of the classes in the training data, the information that will be used to update the model is not as equally represented. During model improvement specific actions will be taken to ensure that the model does not become overfit. Although the fonts used in each of the instances are not available, it is hopeful that there is a combination of font and letter that is not contained in the training data and the model will first be exposed to that combination in either the new data or the validation data. One of the goals of this project is to understand how to create a model then implement a process of continual improvements. Through the partitioning and purposeful exposure to new information, the study will accomplish that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18227b",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f837fb",
   "metadata": {},
   "source": [
    "There are several models that will be explored to evaluate accuracy with complete retraining. \n",
    "\n",
    "* Stochastic Gradient Descent\n",
    "* Naive Bayes\n",
    "* Random Forest\n",
    "* Passive Aggressive\n",
    "* Artificial Neural Network\n",
    "\n",
    "The goal from evaluation of the numerous models is to create model deployment options. Through insepction of incremental learning and batch learning protocols, the project will identify which model is best suited in each environment. Ultimately, the conclusions from this section will allow the organization to answer the following questions:\n",
    "\n",
    "* Which model type is best suited for my data?\n",
    "* Which training protocol is best suited for my organization and problem?\n",
    "* Which model allows me to achieved my desired endstate the fastest, with the least amount of retraining?\n",
    "* How robust is my changing model to changes in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1d6a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial data partitioning training with 50% of the original data\n",
    "X = data1[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']]   \n",
    "y = data1['lettr']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b2c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = new1[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y1 = new1['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b30d0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = new2[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y2 = new2['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dee5aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = new3[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y3 = new3['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea69e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = new4[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y4 = new4['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7fae06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = new5[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y5 = new5['lettr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103cb6a-bea0-4b55-83b4-e8dece59dd2c",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1074ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "sgd_clf = SGDClassifier(loss='hinge',\n",
    "                        learning_rate='optimal',\n",
    "                        early_stopping=True,\n",
    "                        validation_fraction=0.1,  \n",
    "                        n_iter_no_change=20,\n",
    "                        random_state=42)\n",
    "\n",
    "# Train with the initial training set\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions\n",
    "YPred1 = sgd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate initial performance\n",
    "initial_accuracy = accuracy_score(y_test, sgd_clf.predict(X_test))\n",
    "print(f'Initial model accuracy: {initial_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix of intial model\n",
    "cm = confusion_matrix(y_test, YPred1)\n",
    "\n",
    "# define class labels\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "plt.figure(figsize=(10, 7))  # You might adjust the size to fit 26 classes\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Stochastic Gradient Descent Model Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('Actual labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbc8a1",
   "metadata": {},
   "source": [
    "## SGD Model Applied on New Data Without Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions\n",
    "YPred_batch1 = sgd_clf.predict(X1)\n",
    "YPred_batch2 = sgd_clf.predict(X2)\n",
    "YPred_batch3 = sgd_clf.predict(X3)\n",
    "YPred_batch4 = sgd_clf.predict(X4)\n",
    "YPred_batch5 = sgd_clf.predict(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance one each batch without any retraining measures\n",
    "accuracy_batch1 = accuracy_score(y1, YPred_batch1)\n",
    "accuracy_batch2 = accuracy_score(y2, YPred_batch2)\n",
    "accuracy_batch3 = accuracy_score(y3, YPred_batch3)\n",
    "accuracy_batch4 = accuracy_score(y4, YPred_batch4)\n",
    "accuracy_batch5 = accuracy_score(y5, YPred_batch5)\n",
    "\n",
    "print(f\"Accuracy on batch 1: {accuracy_batch1:.4f}\")\n",
    "print(f\"Accuracy on batch 2: {accuracy_batch2:.4f}\")\n",
    "print(f\"Accuracy on batch 3: {accuracy_batch3:.4f}\")\n",
    "print(f\"Accuracy on batch 4: {accuracy_batch4:.4f}\")\n",
    "print(f\"Accuracy on batch 5: {accuracy_batch5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d309b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for each batch\n",
    "plot_confusion_matrix(y1, YPred_batch1, 'SGD Batch 1')\n",
    "plot_confusion_matrix(y2, YPred_batch2, 'SGD Batch 2')\n",
    "plot_confusion_matrix(y3, YPred_batch3, 'SGD Batch 3')\n",
    "plot_confusion_matrix(y4, YPred_batch4, 'SGD Batch 4')\n",
    "plot_confusion_matrix(y5, YPred_batch5, 'SGD Batch 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6634f",
   "metadata": {},
   "source": [
    "## SGD Model With Incremental Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d51ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batches as DataFrames\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "# Define class labels\n",
    "classes = np.unique(np.concatenate([y for _, y in new_batches]))\n",
    "\n",
    "# Initialize data storage for predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize the model to train initially on the first batch\n",
    "sgd_clf.partial_fit(new_batches[0][0], new_batches[0][1], classes=classes)\n",
    "\n",
    "for i, (X_new, y_new) in enumerate(new_batches):\n",
    "    # Predict on the current batch\n",
    "    y_pred = sgd_clf.predict(X_new)\n",
    "    accuracy = accuracy_score(y_new, y_pred)\n",
    "    print(f\"Accuracy for batch {i + 1}: {accuracy:.4f}\") \n",
    "    \n",
    "    # Store predictions and true labels for analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(y_new)\n",
    "\n",
    "    # Update model with the current batch\n",
    "    sgd_clf.partial_fit(X_new, y_new, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Loop through stored predictions and true labels\n",
    "for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plot_confusion_matrix(cm, np.unique(y_true), title=f'SGD Incremental Learning for Batch {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76bd52",
   "metadata": {},
   "source": [
    "## SGD Model With Complete Retraining After Each Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retraining function\n",
    "def retrain_model(current_data_X, current_data_y, new_data_X, new_data_y):\n",
    "    # Combine the new data with the existing data using pandas concat\n",
    "    updated_data_X = pd.concat([current_data_X, new_data_X], ignore_index=True)\n",
    "    updated_data_y = pd.concat([current_data_y, new_data_y], ignore_index=True)\n",
    "    \n",
    "    # Reinitialize the model\n",
    "    new_model = SGDClassifier(loss='hinge', penalty='l2', learning_rate='optimal', random_state=42)\n",
    "    \n",
    "    # Retrain the model on the combined dataset\n",
    "    new_model.fit(updated_data_X, updated_data_y)\n",
    "    \n",
    "    return new_model, updated_data_X, updated_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data storage for predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize training data\n",
    "current_data_X, current_data_y = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define your batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_data_X, new_data_y) in enumerate(new_batches, start=1):\n",
    "    # Retrain the model with the current and new batch of data\n",
    "    sgd_clf, current_data_X, current_data_y = retrain_model(current_data_X, current_data_y, new_data_X, new_data_y)\n",
    "    \n",
    "    # Predict on the new batch and evaluate\n",
    "    new_predictions = sgd_clf.predict(new_data_X)\n",
    "    accuracy = accuracy_score(new_data_y, new_predictions)\n",
    "    print(f\"Retrained model accuracy on batch {i}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(new_predictions)\n",
    "    true_labels.append(new_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd25309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Loop through stored predictions and true labels\n",
    "for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plot_confusion_matrix(cm, np.unique(y_true), title=f'SGD With Retraining for Batch {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f15fe3",
   "metadata": {},
   "source": [
    "# Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "random_forest_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest Model\n",
    "random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "predictions = random_forest_clf.predict(X_test)\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "initial_accuracy = accuracy_score(y_test, random_forest_clf.predict(X_test))\n",
    "print(f'Initial model accuracy: {initial_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c05e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create confusion matrix of intial model\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# define class labels\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "plt.figure(figsize=(10, 7))  # You might adjust the size to fit 26 classes\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('Actual labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92747029",
   "metadata": {},
   "source": [
    "## Random Forest Model Applied on New Data Without Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions using original model\n",
    "YPred_batch1 = random_forest_clf.predict(X1)\n",
    "YPred_batch2 = random_forest_clf.predict(X2)\n",
    "YPred_batch3 = random_forest_clf.predict(X3)\n",
    "YPred_batch4 = random_forest_clf.predict(X4)\n",
    "YPred_batch5 = random_forest_clf.predict(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance one each batch without any retraining measures\n",
    "accuracy_batch1 = accuracy_score(y1, YPred_batch1)\n",
    "accuracy_batch2 = accuracy_score(y2, YPred_batch2)\n",
    "accuracy_batch3 = accuracy_score(y3, YPred_batch3)\n",
    "accuracy_batch4 = accuracy_score(y4, YPred_batch4)\n",
    "accuracy_batch5 = accuracy_score(y5, YPred_batch5)\n",
    "\n",
    "print(f\"Accuracy on batch 1: {accuracy_batch1:.4f}\")\n",
    "print(f\"Accuracy on batch 2: {accuracy_batch2:.4f}\")\n",
    "print(f\"Accuracy on batch 3: {accuracy_batch3:.4f}\")\n",
    "print(f\"Accuracy on batch 4: {accuracy_batch4:.4f}\")\n",
    "print(f\"Accuracy on batch 5: {accuracy_batch5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd417f",
   "metadata": {},
   "source": [
    "## Random Forest Model With Retraining After Each Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_with_new_batch(model, current_X, current_y, new_X, new_y):\n",
    "    # Combine the new data with the existing data using pandas concat\n",
    "    updated_X = pd.concat([current_X, new_X], ignore_index=True)\n",
    "    updated_y = pd.concat([current_y, new_y], ignore_index=True)\n",
    "    \n",
    "    # Re-train the model on the combined dataset\n",
    "    model.fit(updated_X, updated_y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate the initial training data as pandas DataFrame and Series\n",
    "accumulated_X = X_train.copy()\n",
    "accumulated_y = y_train.copy()\n",
    "\n",
    "# Define batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "# Sequentially retrain and evaluate the model with each new batch\n",
    "for i, (new_X, new_y) in enumerate(new_batches, start=1):\n",
    "    # Retrain the model with the current and new batch of data\n",
    "    random_forest_clf = retrain_model_with_new_batch(random_forest_clf, accumulated_X, accumulated_y, new_X, new_y)\n",
    "    \n",
    "    # Update the accumulated data with the new batch using pandas concat\n",
    "    accumulated_X = pd.concat([accumulated_X, new_X], ignore_index=True)\n",
    "    accumulated_y = pd.concat([accumulated_y, new_y], ignore_index=True)\n",
    "    \n",
    "    # Evaluate the retrained model on the test set\n",
    "    accuracy = accuracy_score(y_test, random_forest_clf.predict(X_test))\n",
    "    print(f'Retrained model accuracy after batch {i}: {accuracy:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501901c",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8436154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991b4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ecd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "initial_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Initial model accuracy: {initial_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e9348",
   "metadata": {},
   "source": [
    "## Naive Bayes Model Applied on New Data Without Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions using original model\n",
    "YPred_batch1 = mnb.predict(X1)\n",
    "YPred_batch2 = mnb.predict(X2)\n",
    "YPred_batch3 = mnb.predict(X3)\n",
    "YPred_batch4 = mnb.predict(X4)\n",
    "YPred_batch5 = mnb.predict(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a905c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance one each batch without any retraining measures\n",
    "accuracy_batch1 = accuracy_score(y1, YPred_batch1)\n",
    "accuracy_batch2 = accuracy_score(y2, YPred_batch2)\n",
    "accuracy_batch3 = accuracy_score(y3, YPred_batch3)\n",
    "accuracy_batch4 = accuracy_score(y4, YPred_batch4)\n",
    "accuracy_batch5 = accuracy_score(y5, YPred_batch5)\n",
    "\n",
    "print(f\"Accuracy on batch 1: {accuracy_batch1:.4f}\")\n",
    "print(f\"Accuracy on batch 2: {accuracy_batch2:.4f}\")\n",
    "print(f\"Accuracy on batch 3: {accuracy_batch3:.4f}\")\n",
    "print(f\"Accuracy on batch 4: {accuracy_batch4:.4f}\")\n",
    "print(f\"Accuracy on batch 5: {accuracy_batch5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for each batch\n",
    "plot_confusion_matrix(y1, YPred_batch1, 'Naive Bayes for Batch 1')\n",
    "plot_confusion_matrix(y2, YPred_batch2, 'Naive Bayes for Batch 2')\n",
    "plot_confusion_matrix(y3, YPred_batch3, 'Naive Bayes for Batch 3')\n",
    "plot_confusion_matrix(y4, YPred_batch4, 'Naive Bayes for Batch 4')\n",
    "plot_confusion_matrix(y5, YPred_batch5, 'Naive Bayes for Batch 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050f431",
   "metadata": {},
   "source": [
    "## Naive Bayes Model With Incremental Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51eff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "# Define batches as DataFrames\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for i, (X_new, y_new) in enumerate(new_batches, start=1):\n",
    "    # Update the model with the new batch using partial_fit\n",
    "    mnb.partial_fit(X_new, y_new, classes=classes)\n",
    "    \n",
    "    # Evaluate the updated model on a consistent test set\n",
    "    y_pred = mnb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy after batch {i}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96430c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Loop through stored predictions and true labels\n",
    "for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    plot_confusion_matrix(cm, classes, title=f'Naive Bayes Incremental Learning for Batch {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09669b4b",
   "metadata": {},
   "source": [
    "## Naive Bayes With Complete Retraining After Each Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Multinomial Naive Bayes model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Define classes and original trainging data\n",
    "classes = np.unique(y_train)\n",
    "current_X_train, current_y_train = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define new batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_X, new_y) in enumerate(new_batches):\n",
    "    # Retrain model on the current training data\n",
    "    mnb.fit(current_X_train, current_y_train)\n",
    "\n",
    "    # Predict on the current batch and evaluate\n",
    "    y_pred = mnb.predict(new_X)\n",
    "    accuracy = accuracy_score(new_y, y_pred)\n",
    "    print(f'Accuracy after retraining with batch {i+1}: {accuracy:.4f}')\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(new_y)\n",
    "\n",
    "    # Update the current training dataset with the current batch for the next iteration\n",
    "    # This keeps the model learning cumulatively\n",
    "    current_X_train = pd.concat([current_X_train, new_X], ignore_index=True)\n",
    "    current_y_train = pd.concat([current_y_train, new_y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d63fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, true_labels, classes):\n",
    "    for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'Naive Bayes With Retraining for Batch {i}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the stored predictions and labels\n",
    "plot_confusion_matrices(predictions, true_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32086cc",
   "metadata": {},
   "source": [
    "# Passive Aggressive Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "pac = PassiveAggressiveClassifier(max_iter=1000, random_state=42, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ff9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "pac.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_predictions = pac.predict(X_test)\n",
    "print(f\"Initial accuracy: {accuracy_score(y_test, initial_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d1334",
   "metadata": {},
   "source": [
    "## Passive Aggressive Model With Incremental Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270302c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes\n",
    "classes = np.unique(np.concatenate([y_train] + [y for _, y in new_batches]))\n",
    "\n",
    "# Define new data\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Process each batch for prediction and subsequent training\n",
    "for i, (X_new, y_new) in enumerate(new_batches):\n",
    "    # Apply the model on the current batch to gather predictions\n",
    "    current_predictions = pac.predict(X_new)\n",
    "    current_accuracy = accuracy_score(y_new, current_predictions)\n",
    "    print(f\"Accuracy for batch {i + 1}: {current_accuracy:.4f}\")\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(current_predictions)\n",
    "    true_labels.append(y_new)\n",
    "    \n",
    "    # Use partial fit to update the model with the current batch\n",
    "    pac.partial_fit(X_new, y_new, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, true_labels, classes):\n",
    "    for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'PAC Incremental Training for Batch {i}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the stored predictions and labels\n",
    "plot_confusion_matrices(predictions, true_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76d559",
   "metadata": {},
   "source": [
    "## Passive Aggressive Model With Retraining After Each Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a21161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and original trainging data\n",
    "classes = np.unique(y_train)\n",
    "current_X_train, current_y_train = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_X, new_y) in enumerate(new_batches):\n",
    "    # Retrain model on the current training data\n",
    "    pac.fit(current_X_train, current_y_train)\n",
    "\n",
    "    # Predict on the current batch and evaluate\n",
    "    y_pred = pac.predict(new_X)\n",
    "    accuracy = accuracy_score(new_y, y_pred)\n",
    "    print(f'Accuracy after retraining with batch {i+1}: {accuracy:.4f}')\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(new_y)\n",
    "\n",
    "    # Update the current training dataset with the current batch for the next iteration\n",
    "    current_X_train = pd.concat([current_X_train, new_X], ignore_index=True)\n",
    "    current_y_train = pd.concat([current_y_train, new_y], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, true_labels, classes):\n",
    "    for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'PAC with Retraining for Batch {i}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "        \n",
    "# Call the function to plot confusion matrices after processing all batches\n",
    "plot_confusion_matrices(predictions, true_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918b44c",
   "metadata": {},
   "source": [
    "# Artificial Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2831a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partition Data into new data frames to later simulate new information\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "data_shuffled = data.sample(frac=1, random_state=44).reset_index(drop=True)\n",
    "\n",
    "# Calculate the size of each partition\n",
    "partition_size = int(np.ceil(len(data_shuffled) / 10))\n",
    "\n",
    "# Split the data into 10 equally sized DataFrames\n",
    "data_parts = [data_shuffled.iloc[i * partition_size:(i + 1) * partition_size] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and rename the data frames from the list\n",
    "add1, add2, add3, add4, add5, add6, add7, add8, add9, add10 = data_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = add1[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y6 = add1['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17261069",
   "metadata": {},
   "outputs": [],
   "source": [
    "X7 = add2[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y7 = add2['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d727ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X8 = add3[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y8 = add3['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56029efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X9 = add4[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y9 = add4['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708502a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X10 = add5[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y10 = add5['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66650967",
   "metadata": {},
   "outputs": [],
   "source": [
    "X11 = add6[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y11 = add6['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ff6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X12 = add7[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y12 = add7['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X13 = add8[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y13 = add8['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901555f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X14 = add9[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y14 = add9['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbcd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X15 = add10[['x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr',\n",
    "           'x2ybr', 'xy2br', 'x-ege','xegvy', 'y-ege', 'yegvx']] \n",
    "y15 = add10['lettr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batches as DataFrames\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f866156",
   "metadata": {},
   "source": [
    "## SGD Incremental Learning with Additional Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batches as DataFrames\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]\n",
    "\n",
    "# Define class labels\n",
    "classes = np.unique(np.concatenate([y for _, y in new_batches]))\n",
    "\n",
    "# Initialize data storage for predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize the model to train initially on the first batch\n",
    "sgd_clf.partial_fit(new_batches[0][0], new_batches[0][1], classes=classes)\n",
    "\n",
    "for i, (X_new, y_new) in enumerate(new_batches):\n",
    "    # Predict on the current batch\n",
    "    y_pred = sgd_clf.predict(X_new)\n",
    "    accuracy = accuracy_score(y_new, y_pred)\n",
    "    print(f\"Accuracy for batch {i + 1}: {accuracy:.4f}\") \n",
    "    \n",
    "    # Store predictions and true labels for analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(y_new)\n",
    "\n",
    "    # Update model with the current batch\n",
    "    sgd_clf.partial_fit(X_new, y_new, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc387c",
   "metadata": {},
   "source": [
    "## SGD Retraining With Additional Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff70f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data storage for predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize training data\n",
    "current_data_X, current_data_y = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define your batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_data_X, new_data_y) in enumerate(new_batches, start=1):\n",
    "    # Retrain the model with the current and new batch of data\n",
    "    sgd_clf, current_data_X, current_data_y = retrain_model(current_data_X, current_data_y, new_data_X, new_data_y)\n",
    "    \n",
    "    # Predict on the new batch and evaluate\n",
    "    new_predictions = sgd_clf.predict(new_data_X)\n",
    "    accuracy = accuracy_score(new_data_y, new_predictions)\n",
    "    print(f\"Retrained model accuracy on batch {i}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(new_predictions)\n",
    "    true_labels.append(new_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d888c",
   "metadata": {},
   "source": [
    "## PAC Incremental Training with Additional Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def29043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes\n",
    "classes = np.unique(np.concatenate([y_train] + [y for _, y in new_batches]))\n",
    "\n",
    "# Define new data\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Process each batch for prediction and subsequent training\n",
    "for i, (X_new, y_new) in enumerate(new_batches):\n",
    "    # Apply the model on the current batch to gather predictions\n",
    "    current_predictions = pac.predict(X_new)\n",
    "    current_accuracy = accuracy_score(y_new, current_predictions)\n",
    "    print(f\"Accuracy for batch {i + 1}: {current_accuracy:.4f}\")\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(current_predictions)\n",
    "    true_labels.append(y_new)\n",
    "    \n",
    "    # Use partial fit to update the model with the current batch\n",
    "    pac.partial_fit(X_new, y_new, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9c52a",
   "metadata": {},
   "source": [
    "## PAC With Retraining and Additional Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b545da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and original trainging data\n",
    "classes = np.unique(y_train)\n",
    "current_X_train, current_y_train = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define your batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_X, new_y) in enumerate(new_batches):\n",
    "    # Retrain model on the current training data\n",
    "    pac.fit(current_X_train, current_y_train)\n",
    "\n",
    "    # Predict on the current batch and evaluate\n",
    "    y_pred = pac.predict(new_X)\n",
    "    accuracy = accuracy_score(new_y, y_pred)\n",
    "    print(f'Accuracy after retraining with batch {i+1}: {accuracy:.4f}')\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(new_y)\n",
    "\n",
    "    # Update the current training dataset with the current batch for the next iteration\n",
    "    current_X_train = pd.concat([current_X_train, new_X], ignore_index=True)\n",
    "    current_y_train = pd.concat([current_y_train, new_y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855dc1c2",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0296d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN classifier (Validation Loss (1% point = 93%)\n",
    "ANNClf = MLPClassifier(max_iter=500, learning_rate_init=0.001, hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "#scaler = StandardScaler()\n",
    "#X_trainANN = scaler.fit_transform(X_train)\n",
    "#X_testANN = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "ANNClf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data and evaluate Accuracy\n",
    "YPred5 = ANNClf.predict(X_test)\n",
    "ACC5 = accuracy_score(y_test, YPred5)\n",
    "print(\"Artificial Neural Network (M5): Accuracy Score:\", ACC5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d3603-4294-4f0d-a9a9-f2eaca415f78",
   "metadata": {},
   "source": [
    "## ANN with Incremental Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1S = scaler.transform(X1)\n",
    "#X2S = scaler.transform(X2)\n",
    "#X3S = scaler.transform(X3)\n",
    "#X4S = scaler.transform(X4)\n",
    "#X5S = scaler.transform(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batches and loop though each batch creating predictions\n",
    "#new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "#for i, (X_new, y_new) in enumerate(new_batches, 1):\n",
    "    # Update the model with the new batch\n",
    "#    ANNClf.partial_fit(X_new, y_new, classes=np.unique(y))\n",
    "\n",
    "    # Evaluate the updated model on the new batch\n",
    "  #  new_predictions = ANNClf.predict(X_new)\n",
    " #   new_accuracy = accuracy_score(y_new, new_predictions)\n",
    " #   print(f\"Batch {i} accuracy: {new_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Classification Report\n",
    "print(\"Artificial Neural Network (M5): Classification Report:\")\n",
    "print(classification_report(y_test, YPred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995e6b8-7aa4-4915-9ec0-90a0e386fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes\n",
    "classes = np.unique(np.concatenate([y_train] + [y for _, y in new_batches]))\n",
    "\n",
    "# Define new data\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Process each batch for prediction and subsequent training\n",
    "for i, (X_new, y_new) in enumerate(new_batches):\n",
    "    # Apply the model on the current batch to gather predictions\n",
    "    current_predictions = ANNClf.predict(X_new)\n",
    "    current_accuracy = accuracy_score(y_new, current_predictions)\n",
    "    print(f\"Accuracy for batch {i + 1}: {current_accuracy:.4f}\")\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(current_predictions)\n",
    "    true_labels.append(y_new)\n",
    "    \n",
    "    # Use partial fit to update the model with the current batch\n",
    "    ANNClf.partial_fit(X_new, y_new, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9577e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(ANNClf, true_labels, classes):\n",
    "    for i, (YPred5, y_true) in enumerate(zip(ANNClf, true_labels), start=1):\n",
    "        cm = confusion_matrix(y_true, YPred5, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'ANN Incremental Training for Batch {i}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the stored predictions and labels\n",
    "plot_confusion_matrices(predictions, true_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510a433-3a56-45ee-a1b6-39579895c399",
   "metadata": {},
   "source": [
    "## ANN with Retraining After Each Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88decc98-1545-4522-babc-b0076918d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and original trainging data\n",
    "classes = np.unique(y_train)\n",
    "current_X_train, current_y_train = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_X, new_y) in enumerate(new_batches):\n",
    "    # Retrain model on the current training data\n",
    "    ANNClf.fit(current_X_train, current_y_train)\n",
    "\n",
    "    # Predict on the current batch and evaluate\n",
    "    y_pred = ANNClf.predict(new_X)\n",
    "    accuracy = accuracy_score(new_y, y_pred)\n",
    "    print(f'Accuracy after retraining with batch {i+1}: {accuracy:.4f}')\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(new_y)\n",
    "\n",
    "    # Update the current training dataset with the current batch for the next iteration\n",
    "    current_X_train = pd.concat([current_X_train, new_X], ignore_index=True)\n",
    "    current_y_train = pd.concat([current_y_train, new_y], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de9ced-38c8-4372-a730-c1679a7b2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, true_labels, classes):\n",
    "    for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'ANN with Retraining for Batch {i}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "        \n",
    "# Call the function to plot confusion matrices after processing all batches\n",
    "plot_confusion_matrices(predictions, true_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d565df-a66c-441d-b68b-9a49dcdce059",
   "metadata": {},
   "source": [
    "## ANN Incremental Training with Additional Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f885f-ffd7-4350-abe6-0ec8e4d20c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes\n",
    "classes = np.unique(np.concatenate([y_train] + [y for _, y in new_batches]))\n",
    "\n",
    "# Define new data\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Process each batch for prediction and subsequent training\n",
    "for i, (X_new, y_new) in enumerate(new_batches):\n",
    "    # Apply the model on the current batch to gather predictions\n",
    "    current_predictions = ANNClf.predict(X_new)\n",
    "    current_accuracy = accuracy_score(y_new, current_predictions)\n",
    "    print(f\"Accuracy for batch {i + 1}: {current_accuracy:.4f}\")\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(current_predictions)\n",
    "    true_labels.append(y_new)\n",
    "    \n",
    "    # Use partial fit to update the model with the current batch\n",
    "    ANNClf.partial_fit(X_new, y_new, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc794647-f946-46de-9fd3-caa44aebcda8",
   "metadata": {},
   "source": [
    "## ANN With Retraining and Additional Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f2408-f5b6-417a-988a-6c16098e4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and original trainging data\n",
    "classes = np.unique(y_train)\n",
    "current_X_train, current_y_train = X_train.copy(), y_train.copy()\n",
    "\n",
    "# Define your batches\n",
    "new_batches = [(X1, y1), (X2, y2), (X3, y3), (X4, y4), (X5, y5), (X6, y6), (X7, y7), (X8, y8),\n",
    "              (X9, y9), (X10, y10), (X11, y11), (X12, y12), (X13, y13), (X14, y14), (X15, y15)]\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through each batch\n",
    "for i, (new_X, new_y) in enumerate(new_batches):\n",
    "    # Retrain model on the current training data\n",
    "    ANNClf.fit(current_X_train, current_y_train)\n",
    "\n",
    "    # Predict on the current batch and evaluate\n",
    "    y_pred = ANNClf.predict(new_X)\n",
    "    accuracy = accuracy_score(new_y, y_pred)\n",
    "    print(f'Accuracy after retraining with batch {i+1}: {accuracy:.4f}')\n",
    "\n",
    "    # Store predictions and actual labels for later confusion matrix analysis\n",
    "    predictions.append(y_pred)\n",
    "    true_labels.append(new_y)\n",
    "\n",
    "    # Update the current training dataset with the current batch for the next iteration\n",
    "    current_X_train = pd.concat([current_X_train, new_X], ignore_index=True)\n",
    "    current_y_train = pd.concat([current_y_train, new_y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11dd098-fb28-41f2-99d5-b56fc05e6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, true_labels, classes):\n",
    "    for i, (y_pred, y_true) in enumerate(zip(predictions, true_labels), start=1):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'ANN with Retraining for Batch {i}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "        \n",
    "# Call the function to plot confusion matrices after processing all batches\n",
    "plot_confusion_matrices(predictions, true_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54633d95-316f-4d1f-8a74-1b5e48a1c2f1",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a35bc1d5-2636-4cc1-8c0a-1f1c50d0de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "df280044-8c85-464e-a563-6abe893df276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   x-box   20000 non-null  int64\n",
      " 1   y-box   20000 non-null  int64\n",
      " 2   width   20000 non-null  int64\n",
      " 3   high    20000 non-null  int64\n",
      " 4   onpix   20000 non-null  int64\n",
      " 5   x-bar   20000 non-null  int64\n",
      " 6   y-bar   20000 non-null  int64\n",
      " 7   x2bar   20000 non-null  int64\n",
      " 8   y2bar   20000 non-null  int64\n",
      " 9   xybar   20000 non-null  int64\n",
      " 10  x2ybr   20000 non-null  int64\n",
      " 11  xy2br   20000 non-null  int64\n",
      " 12  x-ege   20000 non-null  int64\n",
      " 13  xegvy   20000 non-null  int64\n",
      " 14  y-ege   20000 non-null  int64\n",
      " 15  yegvx   20000 non-null  int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 2.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   lettr   20000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fa3c2cc6-568c-448f-9d0b-17cfef62ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5675/3672081819.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['lettr'] = pd.Categorical(y['lettr'])\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'lettr' column of the DataFrame y to a categorical data type\n",
    "y['lettr'] = pd.Categorical(y['lettr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66b8a8-98b9-4bfd-b22a-70a343b3be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09909cc7-acc5-4d01-89f3-28d78702998a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df6be9-1c9d-4116-b844-f2255fccdf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14444b7-b5e3-40c1-9e6c-135b5bf48fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19385876-1279-4f40-a2d0-f346a5041cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c082dc-5d52-4f50-bc18-46210a5ee40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0d4d1-751d-4639-bcc7-3271db36a441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10575a4a-7809-4b26-bf32-4cb3e0c8ffdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d96805-135c-4617-8636-52e57ee49b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e467b20-6cf1-4bd7-8ea0-5dc47af646db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2f9e4-43d3-40c3-883d-a7b9165840e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5e034-b922-4dde-8dab-e3162f65b366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0b085-464c-42df-b403-9eca5f9c9757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2f1520c-5b17-49db-be6d-74b644a59815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset from UCI \n",
    "LRec = fetch_ucirepo(id=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2423edb1-09ba-47f0-aaad-8423725fc5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5675/336509493.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data (as pandas dataframes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lettr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# data (as pandas dataframes) \n",
    "X = LRec.data.features \n",
    "y = LRec.data.targets.reshape(-1, 1)\n",
    "Y = y[\"lettr\"]\n",
    "X.info()\n",
    "y.info()\n",
    "Y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d121de-a1f6-4560-9f21-db7423e7cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = LRec.data                   # Feature variables\n",
    "#Y = LRec.target.reshape(-1, 1)  # Target variable reshaped\n",
    "#print(\"Feature names:\", Iris.feature_names)\n",
    "#print(\"Target names:\", Iris.target_names)\n",
    "#print(\"First 5 samples:\")\n",
    "#for i in range(5):\n",
    "#    print(f\"Sample {i+1}: {X[i]} (Class: {Y[i]}, Species: {Iris.target_names[Y[i]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddf8c224-324d-4b9f-a779-3b08cfe30a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DF to Numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5852d544-3b09-4158-aff0-a601342e1d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m XTrain \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 4\u001b[0m yTrain \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      5\u001b[0m XTest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      6\u001b[0m yTest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "XTrain = torch.tensor(X_train.values)\n",
    "yTrain = torch.tensor(y_train.values)\n",
    "XTest = torch.tensor(X_test.values)\n",
    "yTest = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "89974703-e7ee-4865-9b93-ede5be161daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X1, Y1, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train)\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mtensor(y_train)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m      4\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test)\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mtensor(y_test)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).long()\n",
    "X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).long()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\"), print(f\"X_test shape: {X_test.shape}\"), print(f\"y_train shape: {y_train.shape}\"), print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eca9a3b0-45d1-4ba7-9bae-810a66e2f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network with 3 hidden layers\n",
    "class Net(nn.Module):\n",
    "    def __init__(TNN3):\n",
    "        super(Net, TNN3).__init__()\n",
    "        TNN3.fc1 = nn.Linear(4, 8)  # First hidden layer\n",
    "        TNN3.fc2 = nn.Linear(8, 5)  # Second hidden layer\n",
    "        TNN3.fc3 = nn.Linear(5, 3)  # Third hidden layer\n",
    "        TNN3.output_layer = nn.Linear(3, 3)  # Output layer\n",
    "\n",
    "    def forward(TNN3, x):\n",
    "        x = torch.relu(TNN3.fc1(x))\n",
    "        x = torch.relu(TNN3.fc2(x))\n",
    "        x = torch.relu(TNN3.fc3(x))\n",
    "        x = TNN3.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40518e3b-9e63-42f7-aae5-ffbef30b0312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8000x17 and 4x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[1;32m     16\u001b[0m     Loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[1;32m     17\u001b[0m     Loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(TNN3, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(TNN3, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(TNN3\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(TNN3\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(TNN3\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8000x17 and 4x8)"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Net()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training history\n",
    "TrnLoss = []\n",
    "TrnAccu = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    Loss = criterion(outputs, y_train)\n",
    "    Loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    ACC = (predicted == y_train).sum().item() / len(y_train)\n",
    "    TrnLoss.append(Loss.item())\n",
    "    TrnAccu.append(ACC)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/100], Training Accuracy: {ACC:.4f}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed1339-2953-4a0a-8f5e-4f6bc4b52e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da32cda-54d0-475e-a1cd-c33358eb784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f08635-ac5b-4ebc-b31d-ae1cbc032c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d7c9190-c24b-4b1e-bacb-a2a486634297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 10000 entries, 9434 to 15795\n",
      "Series name: lettr\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "10000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 156.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the y DataFrame\n",
    "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd87d17-a40c-4566-876a-341522ebf812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a833d-9432-4ba3-8a9d-2ffdb9ec9c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66e2e2-a1d2-4c93-9880-d9b8a3f3f7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e90b1-4ef4-442c-97d8-ad22a2069d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a3a35-9dee-4e48-9f7b-5c3a0f48b3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76190a3b-2eea-4ee3-86bc-dbf50653daa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4e613-ad18-48c2-ab91-58e73c4c2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Train and Test sets to numpy arrays and then to PyTorch Tensor\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_np).float()\n",
    "#y_train_tensor = torch.tensor(y_train_np).long()\n",
    "X_test_tensor = torch.tensor(X_test_np).float()\n",
    "y_test_tensor = torch.tensor(y_test_np).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac43888-519b-4a97-bacb-5f8a54e14ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Train and Test sets to numpy arrays\n",
    "#X_train_np = X_train.to_numpy()\n",
    "#y_train_np = y_train.to_numpy()\n",
    "#X_test_np = X_test.to_numpy()\n",
    "#y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Convert numpy arrays to PyTorch Tensors\n",
    "#X_train_tensor = torch.tensor(X_train_np).float()\n",
    "#y_train_tensor = torch.tensor(y_train_np).long()\n",
    "#X_test_tensor = torch.tensor(X_test_np).float()\n",
    "#y_test_tensor = torch.tensor(y_test_np).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1a951-dde8-4a52-acbe-b9f3064e4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Train and Test sets to Pytorch Tensor\n",
    "#X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).long()\n",
    "#X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e810d-b451-4ca8-bba6-38b5031ffc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network with 3 hidden layers\n",
    "class Net(nn.Module):\n",
    "    def __init__(TNN3):\n",
    "        super(Net, TNN3).__init__()\n",
    "        TNN3.fc1 = nn.Linear(4, 8)  # First hidden layer\n",
    "        TNN3.fc2 = nn.Linear(8, 5)  # Second hidden layer\n",
    "        TNN3.fc3 = nn.Linear(5, 3)  # Third hidden layer\n",
    "        TNN3.output_layer = nn.Linear(3, 3)  # Output layer\n",
    "\n",
    "    def forward(TNN3, x):\n",
    "        x = torch.relu(TNN3.fc1(x))\n",
    "        x = torch.relu(TNN3.fc2(x))\n",
    "        x = torch.relu(TNN3.fc3(x))\n",
    "        x = TNN3.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752f41b-cd77-4852-9e69-f1f56dc6c08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Net()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training history\n",
    "TrnLoss = []\n",
    "TrnAccu = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_test_tensor)\n",
    "    Loss = criterion(outputs, y_train)\n",
    "    Loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    ACC = (predicted == y_train).sum().item() / len(y_train)\n",
    "    TrnLoss.append(Loss.item())\n",
    "    TrnAccu.append(ACC)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/100], Training Accuracy: {ACC:.4f}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b558122-7918-4f4a-acb7-9f816fed2385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d62feb-938b-4e32-9cfd-94a51e446ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735635d-0692-412a-93d0-90080381bc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e34535-661e-46a7-915a-3a192d7397f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8346f45-0244-4dc4-9c72-a571bd2ae8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190cc86-7fd8-4ff2-8f16-6bcf1338193a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a207c-3502-4ccf-b3e3-e6bc2a87091a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847df5a-24ef-445b-970e-39239d490640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4535a52-ba98-43d7-afdc-97d63d34920d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6f0bc-969b-441d-a723-0aa9c03d3216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25bc36dc-54cf-4207-83af-969f91555430",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df7a3e-4e4d-4b64-be16-e2d5e7b3be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49013d10-c4c7-4c59-978a-39dde6af6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=32*7*7, out_features=128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create an instance of the CNN\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2cfc8-ce32-4a4b-92a3-66164a795183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion for calculating the loss, and define the optimizer for updating the model parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c0bbd-f018-4767-b36a-1ec9987b173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert X_train and y_train to tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Define batch size and create a DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained model (optional)\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21172a5d-0b99-4a46-89b2-06b44b1db221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e45d5-f61a-42eb-aa8d-37ffccbef8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767a9b1-47ce-4dc0-95bb-8c912cddd82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944cee7-776d-4c47-98ec-84de5e095da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
